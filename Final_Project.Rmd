---
title: "Participation in Philadelphia School District (SDP) Kindergartens"
author: "Eric Peterson"
date: "`r Sys.Date()`"
output:
  md_document:
    toc: yes
    #always_allow_html: true
  word_document: default
  html_document:
    toc: yes
  pdf_document:
    toc: yes
always_allow_html: true
---

_Who starts going to public school in Philly?_


# Introduction

Philadelphia public schools are known for their low [performance](https://public.tableau.com/shared/HK7ZPRBQ9?:display_count=y&:origin=viz_share_link&:embed=y) [compared](https://public.tableau.com/shared/K9GTP6CM7?:display_count=n&:origin=viz_share_link) to the rest of the state (Walker, 2023a and Walker, 2023b). Schools throughout the state are underfunded to the extent that the system of that funding has been declared [unconstitutional](https://pubintlaw.org/wp-content/uploads/2023/02/02.07.23-Memorandum-Opinion-Filed-pubintlaw.pdf) (Jubelirer, 2023). Given the reputation of the city's public schools, who in Philadelphia would send their kids to these schools?

One way to measure what families are choosing public schools is to study data about the school catchments. If we know what catchments are sending more kids to school, we can start to predict what resources each school needs. Identifying trends helps identify outliers. Exceptions to a trend can show where more resources are needed to increase enrollment or highlight models to copy elsewhere.

There is evidence that charter schools disproportionately draw from Black and minority communities, controlling for their placement (Cordes and Laurito 2023). In some areas, charter schools serve proportionally more low-income students (Knight, Shin and McMorris 2022). The location of schools has been shown to affect the choice of school (Edwards and Cowen 2022).

I am curious if white families are deciding to send their children to SDP kindergarten or opting out. When white families send their kids to SDP schools, their common interest with disadvantaged communities can be a [force for change](https://www.nytimes.com/2020/08/20/podcasts/nice-white-parents-school.html?showTranscript=1) (Joffe-Walt, 2020). Alternatively, are the pressures to select high-quality schools in the suburbs leading to white families leaving the city (Shapiro 2005)?

I predict that the incentives to look for a school outside of the city and the increasing prevalence of charter schools will disproportionately affect Black families. I expect a negative correlation between more Black residents in a catchment and higher SDP enrollment. I would furter predict that higher median income, a sign that families have the means to move to a desirable suburban neighborhood if they have that desire, will correspond to lower SDP enrollment.


# Methods

## Population

The data available for this project describes the SDP kindergartens in the school years starting in 2016, 2017, 2018 and 2019. Based on the results of a model predicting behavior using the qualities of catchments, I hope to draw some conclusions about the population of families in Philadelphia.


## Data Sources

1. Births data

Births data are available from an internet archive of an old version of the City of Philadephia's webpage.

2. Demographics data

I'll use census tract-level data from the 2010 census to define demographic qualities of SDP catchments at the time of enrollment.

3. School data
The SDP District Performance Office (DPO) provides catchment geometries and grade-level information about SDP schools as part of its open data initiative.


The resulting data set includes a certain number of schools per year.

|Year|Schools|
|----|-------|
|2016|144|
|2017|144|
|2018|128|
|2019|128|

n = 544


## Data preparation

To prepare the database and python virtual environment, please refer to the reademe.md file. To load data to the database, activate your virtual environment and run `python load_data.py`.

The data for this project is a combination of data that is tied to the catchment (school data from DPO) and data that is tied to the census tract (births data and demographics data). I have used spatially weighted averages to distribute the qualities of tracts over the overlapping catchment boundaries.


## Variables of interest


### Response

The variable to predict

1. total_enrolled

Total enrollment of kindergarten students. This includes students enrolled in the school who live in a different catchment.


### Demographics

Key indicators to describe the qualities of the catchment

2. total_births

  Count of live births in the calendar year.

3. pct_white

  The percent of the population that identifies as white alone.

4. pct_black

  The percent of the population that identifies as black alone.

5. median_household_income

  The median income of households in the catchment.

6. median_household_income_diff

  The difference between the highest median household income in the catchment and the lowest median household income in the catchment.
  


### Quality of the school

Attributes specific to what is going on inside the school and when the data is taken

7. catchment_year

  The start year of the school year. For example, the 2016-2017 school year began in late August, 2016. I've coded that year as "2016". This year is stored numerically to allow for this variable to account for a trend over time.

8. cumulative_tenure

  The consecutive years a school leader (principal) has been on the job. The first year is considered 1.

9. pass_percent

  The percentage of third graders scoring a 3 or 4 (a passing grade) on the PSSA English and Language Arts standardized test. 


### Location of the school

Information about the location of the school in the built environment

10. population_density

  The number of residents per acre.

11. contains_charter

  A 0 or 1 boolean set to 1 when there is any charter school offering kindergarten enrollment in the SDP catchment.

12. charter_distance

  The distance in feet from the point location of the SDP school to the nearest charter school offering kindergarten enrollment.

## Model

The flowchart below summarizes the workflow for selecting variables and vetting a final model.

```{r, echo=F, include=F, "Flowchart"}

library(DiagrammeR)

grViz(diagram = "digraph flowchart {
  node [fontname = arial, shape = rectangle]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  tab4 [label = '@@4']
  tab5 [label = '@@5']
  tab6 [label = '@@6']
  tab7 [label = '@@7']
  tab8 [label = '@@8']
  tab9 [label = '@@9']
  tab10 [label = '@@10']
  tab11 [label = '@@11']
  tab12 [label = '@@12']
  tab13 [label = '@@13']
  tab14 [label = '@@14']
  tab15 [label = '@@15']
  tab16 [label = '@@16']
  
  tab1 -> tab2 -> tab3 -> tab12;
  tab12 -> tab4 -> tab13 -> tab5;
  tab5 -> tab6 -> tab14 -> tab8;
  tab8 -> tab9 -> tab16 -> tab10 -> tab11;
  tab7 -> tab4;
  tab8 -> tab7;
  tab9 -> tab7;
  tab10 -> tab7;
  tab13 -> tab7;
  tab6 -> tab7;
  tab11 -> tab15;
  tab16 -> tab7
}
  
  [1]: 'Load raw data to database'
  [2]: 'Transform data in database'
  [3]: 'Import data to R'
  [4]: 'Build a model'
  [5]: 'Review VIF'
  [6]: 'Any collinear variables?'
  [7]: 'Change the model'
  [8]: 'Any patterns in residuals?'
  [9]: 'Any collinearity in plots of pairs of variables?'
  [10]:'Any variables close to having zero predictive value?'
  [11]:'Assess adjusted R-squared and p-level of model F-test'
  [12]:'Run backward stepwise linear regression'
  [13]:'Any variables with low p-level?'
  [14]:'Plot residuals'
  [15]:'Final model'
  [16]:'Run ANOVA between current and previous best model'
  ")

```


```{r, echo=F, "0_Flowsheet"}

grViz("digraph prisma{
    node [shape=box, fontsize = 12, fontname = 'Helvetica', width = 2];
    tab1; tab2; tab3; tab4; tab5; tab6; tab7; tab8;
    tab9; tab10; tab11; tab12; tab13; tab14; tab15; tab16;
    tab17; tab18; tab19; tab20
    
    # create 2 nodes without box around 
    node [shape = point, width = 0, height = 0]
    x1; x2; x3; x4; x5; x6; x7; x8; x9; x10; x11; x12; x13;
    graph [splines=ortho, nodesep=1, dpi = 72]
    
    # Labels
    tab1 [label = 'Load raw data to postgis instance'];
    tab2 [label = 'Transform data in postgis'];
    tab3 [label = 'Load transformed data to R as a dataframe'];
    tab4 [label = 'Run backward stepwise regression']
    tab5 [label = 'Remove variables that do not improve model']
    tab6 [label = 'Add variables that might improve model']
    tab7 [label = 'Build model']
    tab8 [label = 'Run summary']
    tab9 [label = 'Do any coefficients have low p-level?']
    tab10 [label = 'Run VIF']
    tab11 [label = 'Are any VIF above 4?']
    tab12 [label = 'Plot residuals']
    tab13 [label = 'Do residuals support assumptions of linearity?']
    tab14 [label = 'Plot pairs of variables']
    tab15 [label = 'Do plots suggest colinearity?']
    tab16 [label = 'Run ANOVA\nCurrent model vs.\nlast best model']
    tab17 [label = 'No difference with last model?']
    tab18 [label = 'Assess model']
    tab19 [label = 'Is model not significant?']
    tab20 [label = 'Final model']
    
    
    # Edge definitions
    tab1 -> tab2 -> tab3 -> tab4
    tab4 -> tab5 -> tab6 -> tab7
    tab7 -> tab8
    tab8 -> x1 [arrowhead='none']
    x1 -> tab9
    tab9 -> x7 [arrowhead='none']
    x1 -> tab10
    tab10 -> x2 [arrowhead='none']
    x2 -> tab11
    tab11 -> x8 [arrowhead='none']
    x2 -> tab12
    tab12 -> x3 [arrowhead='none']
    x3 -> tab13
    tab13 -> x9 [arrowhead='none']
    x3 -> tab14
    tab14 -> x4 [arrowhead='none']
    x4 -> tab15
    tab15 -> x10 [arrowhead='none']
    x4 -> tab16
    tab16 -> x5 [arrowhead='none']
    x5 -> tab17
    tab17 -> x11 [arrowhead='none']
    x5 -> tab18
    tab18 -> x6 [arrowhead='none']
    x6 -> tab19
    tab19 -> x12 [arrowhead='none']
    x6 -> tab20
    
    x12 -> x11 -> x10 -> x9 -> x8 -> x7 -> x13 [arrowhead='none']
    x13 -> tab5  [constraint=false]
    
    # Make subgraph definition so arrow is horzontal
    subgraph {
      rank = same; tab5; x13;
    }
    subgraph {
      rank = same; tab9; x1; x7;
    }
    subgraph {
      rank = same; tab11; x2; x8;
    }
    subgraph {
      rank = same; tab13; x3; x9;
    }
    subgraph {
      rank = same; tab15; x4; x10;
    }
    subgraph {
      rank = same; tab17; x5; x11;
    }
    subgraph {
      rank = same; tab19; x6; x12;
    }
  }
}
")

```


### Model implementation
    
I'll implement a linear model in R using the `lm` function. The resulting function will predict the natural log of enrollment. I'll consider the natural log of median household income as a possible variable, but other variables will be un-transformed.


# Results

```{r, include=F, results=F, "01_create_connection"}

library(RPostgreSQL)
library(ggplot2)
library(tidyr)
library(dplyr)
library(car)
library(corrplot)
library(dotwhisker)
library(labelled)

dsn_database <- 'sdp'
dsn_hostname <- 'localhost'
dsn_port <- 5432

tryCatch({
    drv <- dbDriver("PostgreSQL")
    print("Connecting to Databaseâ€¦")
    connec <- dbConnect(drv, 
                 dbname = dsn_database,
                 host = dsn_hostname, 
                 port = dsn_port
                 )
    #on.exit(dbDisconnect(connec))
    print("Database Connected!")
    },
    error=function(cond) {
            print("Unable to connect to Database.")
    })

query <- "select * from public.comparison"

df_full <- dbGetQuery(connec, query)

dbDisconnect(connec)

```

I then select the variables of interest to a dataframe. I create a new column for the natural log of `total_enrolled` and remove the original `total_enrolled` column.

```{r, include=F, "02_create_df"}

df <- df_full[c(1, 4, 5, 6, 7, 11, 12, 13, 14, 17, 18, 19, 20, 21)]

# remove data for the one school that goes k-2 and does not have grade 3 standardized test scores
df <- subset(df, complete.cases(df))

# create column that is log(total_enrolled)
df$log_total_enrolled <- log(df$total_enrolled)
df <- df[ , -which(names(df) %in% c("total_enrolled"))]

```




## Initial model

I start with a model that incldues all available variable.

```{r, echo = F, "06_model_iteration"}

model.all <- lm(log_total_enrolled ~ . , data=df)

summary(model.all)

```

I implement the `step` algorithm to programmatically search for a model with the minimum AIC.

```{r, echo=F, message=F, results=F, "07_model_iteration"}

model.step <- step(lm(log_total_enrolled ~ . , data = df), direction = "backward")

summary(model.step)

vif.model.step <- vif(model.step)

vif.model.step

names(which(vif.model.step > 4))

```


The percent black and white residents are highly correlated. In the first iteration I remove `pct_black`. In the second iteration, I remove `pct_white`. In both iterations, the p-level of `pass_percent` climbs above 10%, so I remove that variable in both cases.

I investigate adding back `median_household_income`, but that variable is collinear with several others.

```{r, echo=F, include=F, "08_model_iteration"}

in_step_model <- names(coefficients(model.step))[-1]
in_step_model <- append(in_step_model, "log_total_enrolled")

df.model.1 <- df[ , which(names(df) %in% in_step_model)]

# remove pct_black
df.model.1 <- df.model.1[ , -which(names(df.model.1) %in% c("pct_black"))]

model.1 <- lm(log_total_enrolled ~ . , data = df.model.1)

#summary(model.1)

#vif(model.1)



df.model.2 <- df.model.1[ , -which(names(df.model.1) %in% names(which(summary(model.1)$coefficients[, 4] > 0.01)))]

model.2 <- lm(log_total_enrolled ~ . , data = df.model.2)

#summary(model.2)

#vif(model.2)

#plot(model.2)


in_model.3 <- names(coefficients(model.2))[-1]
in_model.3 <- append(in_model.3, "log_total_enrolled")
in_model.3 <- append(in_model.3, "median_household_income")

df.model.3 <- df[ , which(names(df) %in% in_model.3)]

model.3 <- lm(log_total_enrolled ~ . , data = df.model.3)

#summary(model.3)

#vif(model.3)

# the improvement using median_household_income is significant
#anova(model.3, model.2)

```



```{r, echo = F, include=F, "09_Covariance_of_Income"}


# inequality to raw income
model.income.inequality <- lm(median_household_income_diff ~ median_household_income, data = df.model.3)

ggplot(df.model.3, aes(x=median_household_income, y=median_household_income_diff)) +
  geom_point() +
  ggtitle("Inequality to raw income") +
  geom_abline(intercept = coefficients(model.income.inequality)[1],
              slope = coefficients(model.income.inequality)[2])

summary(model.income.inequality)$adj.r.squared

# charter distance to median income
model.income.distance <- lm(charter_distance ~ median_household_income, data = df.model.3)

ggplot(df.model.3, aes(x=median_household_income, y=charter_distance)) +
  geom_point() +
  ggtitle("Charter Distance to raw income") +
  geom_abline(intercept = coefficients(model.income.distance)[1],
              slope = coefficients(model.income.distance)[2])

summary(model.income.distance)$adj.r.squared

# population density to median income
model.income.density <- lm(population_density ~ median_household_income, data = df.model.3)

ggplot(df.model.3, aes(x=median_household_income, y=population_density)) +
  geom_point() +
  ggtitle("Population density to raw income") +
  geom_abline(intercept = coefficients(model.income.density)[1],
              slope = coefficients(model.income.density)[2])

summary(model.income.density)$adj.r.squared

# pct_white to median_household_income
model.income.white <- lm(pct_white ~ median_household_income, data = df.model.3)

ggplot(df.model.3, aes(x=median_household_income, y=pct_white)) +
  geom_point() +
  ggtitle("Percent white to raw income") +
  geom_abline(intercept = coefficients(model.income.white)[1],
              slope = coefficients(model.income.white)[2])

summary(model.income.white)$adj.r.squared

```

I check the correlation plots of likely variables that could have collinearity with `pct_white` and find multiple variables with considerable collinearity.


```{r, echo=F, include=F, "10_Covariance_of_Others"}

# births to population density
model.dense.births <- lm(total_births ~ population_density, data = df.model.3)

ggplot(df.model.3, aes(x=population_density, y=total_births)) +
  geom_point() +
  ggtitle("Total births to Population Density") +
  geom_abline(intercept = coefficients(model.dense.births)[1],
              slope = coefficients(model.dense.births)[2])

summary(model.dense.births)$adj.r.squared


# charter distance to pct white
model.disance.white <- lm(pct_white ~ charter_distance, data = df.model.3)

ggplot(df.model.3, aes(x=charter_distance, y=pct_white)) +
  geom_point() +
  ggtitle("Percent white to charter distance") +
  geom_abline(intercept = coefficients(model.disance.white)[1],
              slope = coefficients(model.disance.white)[2])

summary(model.disance.white)$adj.r.squared


```




```{r, echo=F, include=F, "11_Covariance_of_white"}

# white percent population density
model.disance.density <- lm(pct_white ~ population_density, data = df.model.3)

ggplot(df.model.3, aes(x=population_density, y=pct_white)) +
  geom_point() +
  ggtitle("Percent white to population density") +
  geom_abline(intercept = coefficients(model.disance.density)[1],
              slope = coefficients(model.disance.density)[2])

summary(model.disance.density)$adj.r.squared

# white percent income inequality
model.white.inequality <- lm(pct_white ~ median_household_income_diff, data = df.model.3)

ggplot(df.model.3, aes(x=median_household_income_diff, y=pct_white)) +
  geom_point() +
  ggtitle("Percent white to inequality") +
  geom_abline(intercept = coefficients(model.white.inequality)[1],
              slope = coefficients(model.white.inequality)[2])

summary(model.white.inequality)$adj.r.squared

```

I rebuild the model to instead retain `pct_black`. After reviewing the standardized parameter plot and the heteroscedasticity of the errors, I decide to remove `cumulative_tenure`. I've determined that choosing a more parsimonious model and a model with slightly less heteroscedasticity is worth the loss of a little predictive power and a slight trend in the leverage plot.

```{r, echo=F, "12_Retain_Black"}

df.model.4 <- df[ , which(names(df) %in% in_step_model)]

# remove pct_white
df.model.4 <- df.model.4[ , -which(names(df.model.4) %in% c("pct_white"))]

model.4 <- lm(log_total_enrolled ~ . , data = df.model.4)


# remove statistically insignificant parameter pass_percent
df.model.5 <- df.model.4[ , -which(names(df.model.4) %in% names(which(summary(model.4)$coefficients[, 4] > 0.01)))]

model.5 <- lm(log_total_enrolled ~ . , data = df.model.5)

#summary(model.5)


# remove cumulative tenure after reviewing the standardized coefficient plot.
df.model.6 <- df.model.5[ , -which(names(df.model.5) %in% c("cumulative_tenure"))]

model.6 <- lm(log_total_enrolled ~ . , data = df.model.6)

#summary(model.6)

#anova(model.6, model.5)

#plot(model.6)


```



```{r, include=F, "13_Covariance_of_Black"}

# black percent population density
model.black.density <- lm(pct_black ~ population_density, data = df.model.5)

ggplot(df.model.5, aes(x=population_density, y=pct_black)) +
  geom_point() +
  ggtitle("Percent black to population density") +
  geom_abline(intercept = coefficients(model.black.density)[1],
              slope = coefficients(model.black.density)[2])

summary(model.black.density)$adj.r.squared

# black percent income inequality
model.black.inequality <- lm(pct_black ~ median_household_income_diff, data = df.model.5)

ggplot(df.model.5, aes(x=median_household_income_diff, y=pct_black)) +
  geom_point() +
  ggtitle("Percent black to inequality") +
  geom_abline(intercept = coefficients(model.black.inequality)[1],
              slope = coefficients(model.black.inequality)[2])

summary(model.black.inequality)$adj.r.squared

# black percent charter distance
model.black.distance <- lm(pct_black ~ charter_distance, data = df.model.5)

ggplot(df.model.5, aes(x=charter_distance, y=pct_black)) +
  geom_point() +
  ggtitle("Percent black to charter distance") +
  geom_abline(intercept = coefficients(model.black.distance)[1],
              slope = coefficients(model.black.distance)[2])

summary(model.black.distance)$adj.r.squared


```





## Final model

### Summary

I'm left with my sixth and final iteration as a selected model.

The overall p-value of the model is quite small. The adjusted R-squared is 0.59, which means the model has enough predictive power to be relevant. The p-value for each coefficient is small (< 0.001 for all coefficients), so I am confident that no coefficient could be zero.

The residual plots look mostly random. There is notable deviation from a normal distribution in the QQ-line. Scale Location shows a touch of hetrosecasdicisty. Overall, plots of the residuals do not have striking trends.


```{r, echo=F, "14_final_model_summary"}

model.final <- model.6
df.final <- df.model.6

# list the names of the variables included
#names(coefficients(model.final))[-1]

summary(model.final)

# including median_household_income improves the residuals
par(mfrow = c(2, 2))
plot(model.final)
par(mfrow = c(1, 1))

```

### Variable Inflation Factors

The variable inflation factors for all variables are low and comparable. I am confident that I've avoided collinearity because I've reviewed the grid of plots of all variables against one another and investigated likely co-correlates.

```{r, echo=F, "15_final_model_vif"}

vif.final <- vif(model.final)

# set the margins to not cut off my variable names
par(mar=c(2,13,4,2))
# create horizontal bar chart to display each VIF value
barplot(vif.final, main = "VIF Values", horiz = TRUE, col = "steelblue", las = 1, xlim = c(0, 6))
# add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)

```


### Comparison to Enrollment Alone

We know from preliminary analysis that births alone can explain >40% of the variation in enrollment in a catchment. One comprehensive check would be to confirm that the final model is significantly more predictive than the the births alone model. Running an ANOVA check between the two models, we can see that we do not have evidence to support the hypothesis that both models are equally predictive.


```{r, include=FALSE, echo=F, "16_Births_Enrollment"}

model.births.enrolment <- lm(log_total_enrolled ~ total_births, data = df.model.2)

ggplot(df.model.2, aes(x=total_births, y=log_total_enrolled)) +
  geom_point() +
  ggtitle("Total births to enrollment") +
  geom_abline(intercept = coefficients(model.births.enrolment)[1],
              slope = coefficients(model.births.enrolment)[2])

summary(model.births.enrolment)$adj.r.squared

anova(model.births.enrolment, model.final)

```

### Standardized error

Births is clearly the most influential variable. Income inequality ranks highly, above percent black and charter distance. Population density and the trend over time are significant but contribute less strongly to the model.

```{r, echo=F, "17_Standardized_Error"}

z.score=function(x){
  z=(x-mean(x))/sd(x)
  return(z)
}

df.final.z <- data.frame(apply(cbind(df.final[names(df.final)]), 2, FUN=z.score) )

names(df.final.z) <- names(df.final)

model.final.z <- lm(log_total_enrolled ~ . , data = df.final.z)

dwplot(model.final.z)

sort(abs(coefficients((model.final.z))[-1]), decreasing = TRUE)

```

### Robustness

This model is generally robust. While `total_births` is by far the most predictive variable, the additional variables together boost the predictive power by more almost 15 percentage points. The final model has a low p-level from its F-test and a low p-value for each of its coefficients. Its residual plots support the assumptions of linearity of the model. A review of variables has minimized colinearity. This model can tell us something useful.


# Discussion

The model supports the hypothesis that more white families in a catchment is correlated with higher SDP kindergaten enrollment. Although the final model does not include percentage of white residents, we can note the strong inverse relationship between percent white and percent black and reason that these varibles have opposite effects. This was borne out in preliminary analysis.

  While median household income was not included in the final model, the income inequality measure was the second strongest variable in the model. The inequality measure displaced raw income due to collinearity. This suggests that having income is not sufficient to move out of your catchment or the city. There must be a motivating factor, like the desire to not send a child to the local school with children of families with significantly different income.

  More births in a catchment was somewhat obviously positively correlated with enrollments. This variable has an outsized strength in the model. This suggests that families have already made decisions about their child's education by moving to the catchment of the school of their choice before their child is born. The strong correlation could point to the inertia of the "default" of attending public school.

  The distance a school is from the nearest charter school is positively correlated with enrollments. When alternative schools are farther away, that creates a barrier to choosing one of those alternatives.

  More dense neighborhoods correspond to more enrollments. The effect of distance to the nearest school would be magnified where walking is the norm. I would suggest that this correlation gets at some intangible piece of culture rather than a direct effect of the built environment. When families are physically closer to their neighbors, they might be more likely to support a public service like SDP kindergarten.

  As years go by, there is a negative correlation with enrollment. Something is happening over time that enrollment is decreasing. While the passage of time is not the direct cause, this variable could represent the change over time in public safety, the cost of living, or gentrification by childless households.

  I'll note that this model predicts the natural log of enrollments - not the raw enrollments. This suggests that these correlations are exponential. Each effect is magnified at higher enrollment counts.


# Conclusion

The finding that higher percentage of black residents in a catchment is correlated with lower SDP kindergarten enrollment might seem counter intuitive. It is possible that, as found in New York, White families' behavior is unchanged - Black families' selection away from public school is responsible for the positive correlation between percent White and enrollment. Regardless, the fact that White families are choosing to be incolved in public schools creates opportunity to build a coalition including middle-class white families to lobby for positive change in education for the city.

The relative strength of difference in income and distance to the closest charter school, and perhaps population density, support the claim that alternatives matter. When a family has an option about where to send their child to school - whether by affording tuition to a private school, having access to a charter school, or by having the wealth and income to move away - they often take that option. A given family's attitude towards their current school selection wouldn't show up in this analysis unless they had the ability to choose an alternative.

The relative strength of the correlation between income inequality and enrollment suggests to me one of two things. First, where a catchment is on a boundary, families might try to send their child to a school on the other side of the boundary. Second, families with means might be motivated against sending their child to the local SDP kindergarten.

The troubling trend over time of declining SDP kindergarten enrollment is probably catching the effects of several trends over time. This is a sign that many families do not see the city as a hospitable environment to raise children. This is a trend that can be reversed.


```{r, include=F, ""}



```

# References

Cordes, Sarah A. and Laurito, Agustina. (2023) _The effects of charter schools on neighborhood and school segregation: Evidence from New York City_ [Online]. Available at: https://www.tandfonline.com/doi/abs/10.1080/07352166.2022.2155525 (Accessed: 19 April 2023)

Edwards, DS and Cowen, J. (2022) _The Roles of Residential Mobility and Distance in Participation in Public School Choice_ [Online]. Available at: https://files.eric.ed.gov/fulltext/ED624270.pdf (Accessed: 22 April 2023)

Joffe-Walt, Chana. (2020) _Episode Five: We Know It When We See It_ [Online]. Available at: https://www.nytimes.com/2020/08/20/podcasts/nice-white-parents-school.html?showTranscript=1 (Accessed: 19 April 2023)

Jubelirer, Renee Cohn. (2023) _Memorandum Opinion_ [Online]. Available at: https://pubintlaw.org/wp-content/uploads/2023/02/02.07.23-Memorandum-Opinion-Filed-pubintlaw.pdf (Accessed: 19 April 2023)

Knight, DS, Shin, J and McMorris, C. (2022) _Student Mobility between Charter and Traditional Public School Sectors: Assessing Enrollment Patterns among Major Charter Management Organizations in Texas_ [Online].  Educ. Sci. 2022, 12(12), 915 Available at: https://doi.org/10.3390/educsci12120915 (Accessed: 22 April 2023)

Oster, Emily. (2022) _Back to School Q&A: Redshirting, School Ratings, Reading, and Montessori_ [Online]. Available at: https://www.parentdata.org/p/back-to-school-q-and-a-redshirting (Accessed: 19 April 2023)

Rothstein, Richard. (2017) _The Color of Law_. New York: Liveright.

Shapiro, Thomas M. (2005) _The Hidden Cost of Being African American: How Wealth Perpetuates Inequality_. Oxford University Press.

Walker, Michael. (2023) _PA_K12_Achievement_v2_ [Online]. Available at: https://public.tableau.com/shared/HK7ZPRBQ9?:display_count=y&:origin=viz_share_link&:embed=y (Accessed: 19 April 2023)

Walker, Michael. (2023) _PA_K12_Achievement_v2_ [Online]. Available at: https://public.tableau.com/shared/K9GTP6CM7?:display_count=n&:origin=viz_share_link (Accessed: 19 April 2023)


# Appendix

## Demographics data

  I have considered using census block group-level data from 2010 census. That data is not immediately available through the census.gov API. Because a major goal of this project for me is the reproduceability of the results and the future use of this dataset for future projects, I've decided to prefer the tract-level data.

  I have also considered using American Community Survey census tract-level data, 5-year estimates from 2010 and 2015. After confirming that the differences over 5 years were not especially meaningful, and that the deltas of qualities of catchments over that 5-year period were not meaningful, I decided to prefer the precision of the census 2010 data.

## Assumptions made in data preparation

### Births

  I've decided to use a five-year offset between births and enrollment because that algorithm produces a 75% overlap. A child is eligible for kindergarten at 5 years old in September. Births data is, presumably, calculated by calendar year starting in January.

|Month|Births Year|Kindergarten Year|
|-|-|
|01|2011|2016|
|02|2011|2016|
|03|2011|2016|
|04|2011|2016|
|05|2011|2016|
|06|2011|2016|
|07|2011|2016|
|08|2011|2016|
|09|2011|2017|
|10|2011|2017|
|11|2011|2017|
|12|2011|2017|

  I assume that births are distributed over months equally. This assumption can produce a great deal of variation. Births might change significantly year to year because of random variability month to month. We could be counting a significant number of births in November 2011 as kindergarten year 2016 that will show up as enrollments in kindergarten year 2017.

  I assume that "redshirting" is uniform over years.

  Families have latitude in when they decide to send children to school. A child born in August 2011 could go to school when they are 6 instead of 5. Commentators have started using ["redshirt"](https://www.parentdata.org/p/back-to-school-q-and-a-redshirting) to describe this practice (Oster, 2022). I think it's reasonable to assume that the redshirts from 2016 to 2017 will take the seats of the redshirts from 2017 to 2018.


  I have assumed that births are equally distributed over each census tract. This assumption will cause variability and will lead to unnecessary variability where population density is not uniform.

### Charter Distance

I have assumed that schools in 2016 are in the same locations that they were in in 2017. DPO begn publishing gps coordinates of schools starting in 2017. While this assumption is not rigorous, it facilitates comparison of the full dataset.

### Median Household Income

I have ignored any negative value from the computation of median household income.

Median household income data shows that catchments with the highest median household income have a significantly higher participation rate. This non-linear correlation reduces this variable's effectiveness as a predictor.

Below is a set of box plots built comparing the ratio of (enrollments / births five years ago) grouped by income quantile, as available from the 2010 acs 5-year estimates.

```{r, include=F, echo=F, "18_Compute_Percentiles"}

library(tidycensus)

acs2010 <- load_variables("2010", "acs5", cache=TRUE)
#View(acs2010)

# B00002_001 Total housing units
# B19013_001 Household income

tract_income  <- get_acs(
  geography = "tract",
  variables = c("B19013_001", # Household income
                "B01003_001"), # Total population
  state = "PA",
  county = '101')

#summary(tract_income)
#head(tract_income)


data_frame=tract_income[,-c(2,5)] # Remove unneeded column referring to the margin of error and name
datatibble=as_tibble(data_frame)
df.acs <- pivot_wider(datatibble, names_from = variable, values_from = estimate) 
#head(df.acs)

# rename the columns to be something more friendly
names(df.acs)[c(2:3)]=c("Population", "HHincome")

df.acs$Population[df.acs$Population == 0] <- NA
df.acs=subset(df.acs, complete.cases(df.acs))

#head(df.acs)

#summary(df.acs)
df.acs.freq <- as.vector(rep.int(df.acs$HHincome, df.acs$Population))

#head(df.acs.freq)

#summary(df.acs.freq)

hist(df.acs.freq)

q <- seq(0, 1, .1)

quantiles_result <- quantile(df.acs.freq, q)
quantiles_result

# I brute forced this but I wish I was able to do it programmatically
quantiles <- unname(quantiles_result)

#quantiles
```

```{r, echo=F, include=F, "19_Plot_Income_Quantiles"}

df.income <- df_full[c(3, 7)]

df.income$quantile10 <- rep(NA, nrow(df.income))

df.income$quantile10[df.income$median_household_income < quantiles[1]] <- 10
df.income$quantile10[df.income$median_household_income >= quantiles[1] & df.income$median_household_income < quantiles[2]] <- 20
df.income$quantile10[df.income$median_household_income >= quantiles[2] & df.income$median_household_income < quantiles[3]] <- 30
df.income$quantile10[df.income$median_household_income >= quantiles[3] & df.income$median_household_income < quantiles[4]] <- 40
df.income$quantile10[df.income$median_household_income >= quantiles[4] & df.income$median_household_income < quantiles[5]] <- 50
df.income$quantile10[df.income$median_household_income >= quantiles[5] & df.income$median_household_income < quantiles[6]] <- 60
df.income$quantile10[df.income$median_household_income >= quantiles[6] & df.income$median_household_income < quantiles[7]] <- 70
df.income$quantile10[df.income$median_household_income >= quantiles[7] & df.income$median_household_income < quantiles[8]] <- 80
df.income$quantile10[df.income$median_household_income >= quantiles[8] & df.income$median_household_income < quantiles[9]] <- 90
df.income$quantile10[df.income$median_household_income >= quantiles[9]] <- 90

df.income$quantile10 <- as.factor(df.income$quantile10)

ggplot(df.income, aes(x=quantile10, y=ratio, group=quantile10, color=quantile10)) +
  coord_flip() +
  geom_boxplot()
```

### Median Household Income differnce

To be considered in-catchment, the census tract must be 10% in the catchment. This removes trivial overlaps of tracts with catchments.

I've tested changing the overlap threshold to 5% or 30%. Raising the threshold reduces the range and variability while decreasing the threshold increases the range and variability, unsurprisingly. However, those changes do not change the shape or trend of the data. I chose 10% as the threshold because it represents a middle ground of the distribution.
  
### Pass Percent

Because of the COVID-19 pandemic, standardized test results are not available for the 2019-2020 school year. I've chosen to standardized test the most recent year of completed test scores. My reasoning, besides necessity, is that families will have access to the most recent year of scores when deciding where to send their child. This assumption could be wrong in several ways. First, the quality of academics at the school in the year the child attends could be most important. Second, the accumulation of scores could be more important than any individual score. In that case, an average over time would be more helpful. Third, the performance of the kindergarten class when it is tested in third grade could be the more important variable.

### Census demographics

When computing census information for SDP catchments, I have weighted the values by space and population. First, I use the area of overlap to allocate the proportion of the population to consider in-catchment from each census tract. Second, I compute how much of the variable (e.g. percent white, median household income) to consider in-catchment. Finally, I sum the variables at the catchment level, where I compute a proportion where necessary (i.e. percent white).

### Enrollment

 I am assuming that all of the kindergarteners are enrolled at their catchment school. While I think this is a reasonable assumption, it glosses over an important difference that has a real consequence on the robustness of any model.
 
### Cumulative Tenure

This variable should probably be changed to a scale starting at zero, not one, in order to measure the effect of that one additional year more effectively.

## Limitations and Extensions

A major issue with analyzing behavior aggregated at the catchment level is that we do not have a way to analyze the behavior of subsets of families. While the births data provides crosstabs by race, the enrollment data from DPO does not provide race crosstabs for kindergarten enrollment. That limitation of the data hampers this analysis.

One possibility to answer questions relevant to who is enrolling in SDP kindergarten is to conduct a survey of families. This would provide a level of granularity that public data analysis cannot.

A further hindrance is the availability of data over time. The latest birth data is 2014, conveniently five years before the last year of enrollment before the COVID-19 pandemic. The best way to increase this sample size would be to scale it over more years. The data model and the analysis in this workbook are built to easily accommodate additional years if/when they become available.

One possible avenue to address this consideration is to request births data from the city. It may be possible to have births coded specifically by catchment, which would reduce variance due to the presumably random pattern of births in census tracts.

This analysis could be extended to include more variables from the census, like percent of residents who are Hispanic and home value. The City of Philadelphia provides [various](https://controller.phila.gov/philadelphia-audits/progressphl/#/scorecards) [indecies](https://phl.maps.arcgis.com/home/item.html?id=e69f51885f4f4744b27a88a3901be0fd) that might be useful to capture more information about the built environment.